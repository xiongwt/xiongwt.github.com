 DRBD安装配置笔记 虚拟机环境：centos 6.2  
主服务器：DRBD01   192.168.110.22  
备服务器：DRBD02   192.168.110.23

###1、DRBD安装 1.1、安装依赖包：
安装DRBD依赖包  

    yum -y install gcc kernel-devel kernel-headers flex  

下载安装drbd源码包：  

    wget http://oss.linbit.com/drbd/8.4/drbd-8.4.1.tar.gz  
    tar xzf drbd-8.4.1.tar.gz  
    cd drbd-8.4.1  
    ./configure --prefix=/usr/local/drbd --with-km     
    make KDIR=/usr/src/kernels/2.6.18-164.el5-x86_64/   
     #根据自己服务器的信息更改
    make install  
    mkdir -p /usr/local/drbd/var/run/drbd  
    cp /usr/local/drbd/etc/rc.d/init.d/drbd /etc/rc.d/init.d  
    chkconfig --add drbd  
    chkconfig drbd on  
     #KDIR=中指定的系统内核源码路径，根据实际情况设置

###1.2、安装drbd模块:
    cd drbd  
    make clean   
    make KDIR=/usr/src/kernels/2.6.18-164.el5-x86_64/    
     #根据实际情况更改  
    cp drbd.ko /lib/modules/2.6.18-164.el5/kernel/lib/       
     #uname -r   <-2.6.18-164.el5
    depmod   
###2、DRBD配置
    fdisk /dev/sdb      
     #挂载一块新硬盘，也可以选择一块空的硬盘  
     #把这个块硬盘设置为主分区  
    mkfs -t ext3 /dev/sdb1  #格式化此硬盘  
    partprobe  # 使kernel重新读取分区表  

 2.1、配置global_common.conf
 
     vim /usr/local/drbd/etc/drbd.d/global_common.conf  global {  
            usage-count yes;  
            # usage-count参数其实只是为了让linbit公司收集目前drbd的使用情   
            况.当drbd在安装和升级的时候会通过http协议发送信息到linbit公司  
            的服务器上面  
            # minor-count dialog-refresh disable-ip-verification
    }
     # 以上为全局配置项
    common {
            protocol C;  
           #protocol A ：写I/O到达本地磁盘和本地的TCP发送缓冲区后，  
           返回操作成功  
           #protocol B ：写I/O到达本地磁盘和远程节点发送缓冲区后，  
           返回操作成功  
           #protocol C ：使用DRBD的第三方协议，写I/O到达本地磁盘和远程节  
           点的磁盘后，返回操作成功  
            handlers {     #默认drbd的库文件，用来定义一系列处理器，  
            用来回应特定事  
                    pri-on-incon-degr "/usr/local/drbd/lib/drbd/notify-pri-on-incon-degr.sh;   
                    /usr/local/drbd/lib/drbd/notify-emergency-reboot.sh; echo b > /proc/sysrq-trigger ; reboot -f";  
                    pri-lost-after-sb "/usr/local/drbd/lib/drbd/notify-pri-lost-after-sb.sh;  
                    /usr/local/drbd/lib/drbd/notify-emergency-reboot.sh; echo b > /proc/sysrq-trigger ; reboot -f";  
                    local-io-error "/usr/local/drbd/lib/drbd/notify-io-error.sh;   
                    /usr/local/drbd/lib/drbd/notify-emergency-shutdown.sh; echo o > /proc/sysrq-trigger ; halt -f";  
                    # fence-peer "/usr/lib/drbd/crm-fence-peer.sh";     
                    《---开启fence功能  
                    # split-brain "/usr/lib/drbd/notify-split-brain.sh root";     
                    《--防止脑裂  
                    # out-of-sync "/usr/lib/drbd/notify-out-of-sync.sh root";     
                    《----同步写入，因不是主主，所以不用开启。  
                    # before-resync-target "/usr/lib/drbd/snapshot-resync-target-lvm.sh -p 15 -- -c 16k";  
                    # after-resync-target /usr/lib/drbd/unsnapshot-resync-target-lvm.sh;  
            }  
     #红色部分需要根据自己的实际路径更改，其他的地方一样。  
            startup {  
                    # wfc-timeout degr-wfc-timeout outdated-wfc-timeout wait-after-sb  
                     wfc-timeout 120;     
                     #该选项设定一个时间值，单位是秒。在启用DRBD块时，初始化脚本drbd会阻塞启动进程的运行，  
                       直到对等节点的出现。该选项就是用来限制这个等待时间的，默认为0，即不限制，永远等待  
                      degr-wfc-timeout 120;   
                      #选项也设定一个时间值，单位为秒。也是用于限制等待时间，只是作用的情形不同：  
                      它作用于一个降级集群（即那些只剩下一个节点的集群）在重启时的等待时间  
            }  
    
            options {  
                    # cpu-mask on-no-data-accessible
            }
    
            disk {
                    # size max-bio-bvecs on-io-error fencing disk-barrier disk-flushes
                    # disk-drain md-flushes resync-rate resync-after al-extents
                    # c-plan-ahead c-delay-target c-fill-target c-max-rate
                    # c-min-rate disk-timeout
                    #on-io-error选项：此选项设定了一个策略，  
                    如果底层设备向上层设备报告发生I/O错误，将按照该策略进行处理。有效的策略包括：     
                    #pass_on：把I/O错误报告给上层设备。如果错误发生在primary节点，把它报告给文件系统，  
                    由上层设备处理这些错误（例如，它会导致文件 系统以只读方式重新挂载），它可能会导致drbd停止提供服务；  
                    如果发生在secondary节点，则忽略该错误（因为secondary节点没有上层设 备可以报告）。  
                    该策略曾经是默认策略，但现在已被detach所取代。  
                    #call-local-io-error：调用预定义的本地local-io-error脚本进行处理。  
                    该策略需要在resource配置段的 handlers部分，预定义一个相应的local-io-error命令调用。  
                    该策略完全由管理员通过local-io-error命令（或脚本）调用 来控制如何处理I/O错误。  
                    #detach：发生I/O错误的节点将放弃底层设备，以diskless mode继续工作。  
                    在disklessmode下，只要还有网络连接，drbd将从secondary node读写数据，而不需要failover。  
                    该策略会导致一定的损失，但好处也很明显，drbd服务不会中断。官方推荐和默认策略  
     
    
                    #fencing选项：该选项设定一个策略来避免split brain的状况。有效的策略包括：      
                    #dont-care：默认策略。不采取任何隔离措施。      
                    #resource-only：在此策略下，如果一个节点处于split brain状态，它将尝试隔离对等端的磁盘。  
                    这个操作通过调用fence-peer处理器来实现。fence-peer处理器将通过其它通信路径到达对等   
                    节点，并在这个对等节点上调用drbdadm outdate res命令。    
                   #resource-and-stonith：在此策略下，  
                   如果一个节点处于split brain状态，它将停止I/O操作，并调用fence-peer处理器。  
                   处理器通过其它通信路径到达对等节点，并在这个对等节点上调用drbdadm outdate res命令。  
                   如果无法到达对等节点，它将向对等端发送关机命令。一旦问题解决，I/O操作将重新进行。  
                   如果处理器失败，你可以使用resume-io命令 来重新开始I/O操作  
            }
    
            net {
                    # protocol timeout max-epoch-size max-buffers unplug-watermark
                    # connect-int ping-int sndbuf-size rcvbuf-size ko-count
                    # allow-two-primaries cram-hmac-alg shared-secret after-sb-0pri
                    # after-sb-1pri after-sb-2pri always-asbp rr-conflict
                    # ping-timeout data-integrity-alg tcp-cork on-congestion
                    # congestion-fill congestion-extents csums-alg verify-alg
                    # use-rle
                    #cram-hmac-alg "sha1";  
                    #shared-secret "Myhttpd";   #DRBD同步时使用的验证方式和密码
            }
    }   
###2.2、配置r0资源：
创建r0资源：  
    vim /usr/local/drbd/etc/drbd.d/r0.res  
    
    写入文件内容：
    resource r0{     #资源名
            on DRDB01{    #主机说明，必须以on开头，后面主机名
                    device          /dev/drbd1;   #drbd生成的磁盘分
                    disk            /dev/sdb1;       #drbd使用的磁盘分区
                    address         192.168.110.22:7788;    #通信协议端口
                    meta-disk       internal;      #drbd的元数据存储方式
            }
            on DRDB02{
                    device          /dev/drbd1;
                    disk            /dev/sdb1;
                    address         192.168.110.23:7788;
                    meta-disk       internal;
            }
    }
需要把上面用到的防火墙7788端口打开，这个端口是自定义的，如果嫌麻烦可以直接关掉防火墙。  
说明：  
device  是自定义的物理设备的逻辑路径  
disk        是磁盘设备，或者是逻辑分区  
address   是机器监听地址和端口  
meta-disk   元数据存储方式  
### 2.3、建立resource
    modprobe drbd      //载入 drbd 模块   
    lsmod | grep drbd    //确认 drbd 模块是否载入   
    dd if=/dev/zero of=/dev/sdb1 bs=1M count=100    //把一些资料塞到 sdb1 內 (否则 create-md 时会报错)   
    drbdadm create-md r0         //建立 drbd resource   
    drbdadm up r0  

我遇到的问题： 

    1. [root@backupNode /]# drbdadm up r0  
	2. 1: Failure: (104) Can not open backing device.  
	3. Command 'drbdsetup attach 1 /dev/sda3 /dev/sda3 internal' terminated with exit code   

原因是我之前已经挂在了/dev/sdb1，需要先卸载/dev/sdb1设备，解决办法:  
umount /dev/sdb1    
问题解决。  
 注：以上每一步骤，都需要在主备服务器上进行配置设置。备服务器DRBD02安装配置和主服务器一致。 2.4、设置Primary Node  
将DRBD01设为主服务器(primary node)，在DRBD01上执行：  

drbdadm primary --force r0  

查看drbd状态：

    [root@DRDB01 ~]# cat /proc/drbd  
    version: 8.4.1 (api:1/proto:86-100)  
    GIT-hash: 91b4c048c1a0e06777b5f65d312b38d47abaea80 build by root@DRDB01, 2013-01-06 11:31:41
    
    1: cs:StandAlone ro:Primary/Unknown ds:UpToDate/DUnknown   r-----
        ns:4192852 nr:128 dw:132796 dr:4193058 al:49 bm:256 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0

已经变成了主服务器。

### 2.5、创建DRBD文件系统
上面已经完成了/dev/drbd1的初始化，现在来把/dev/drbd1格式化成ext3格式的文件系统,在DRBD01上执行：  

    mkfs.ext3 /dev/drbd1  

然后，将/dev/drbd1挂载到之前创建好的/DRBD目录：  

    mount /dev/drbd1 /DRBD/  

现在只要把数据写入/DRBD目录，drbd即会立刻把数据同步到DRBD02的/dev/sdb1分区上了。  

###2.6、DRBD同步测试
1、首先，在主服务器上先将设备卸载，同时将主服务器降为备用服务器：  
umount /dev/drbd1  
drbdadm secondary r0    #将主服务器降为备用服务器  
2、然后，登录备用服务器，将备用服务器升为主服务器，同时挂载drbd1设备到 /DRBD目录：目录：  
ssh DRBD02  
drbdadm primary r0  
mount /dev/drbd1 /DRBD/   
3、最后，进入/DRBD目录，就可以看到之前在另外一台机器上放入的数据了，如果没有看到，说明同步失败！  
目录，就可以看到之前在另外一台机器上放入的数据了，如果没有看到，说明同步失败！  
后记：此时的DRBD能实现数据的同步，但是主服务器异常后，备服务器不能自动投入使用。  

